# Review Specification/Plan Command

## Command

`/review-this <path-to-spec-or-plan>`

## Description

Performs a comprehensive CTO-level review of specifications, implementation plans, or architectural designs. Provides harsh but fair critique focusing on completeness, best practices, and hidden assumptions.

**The complete review (including next steps) is automatically saved to a timestamped file** in `docs/{git-branch-name}/reviews/REVIEW-{stage}-{iteration}-{status}-{timestamp}.md` for easy developer reference.

**Reviews are version controlled** and stay with the specs/plans they review, providing chronological context for design decisions.

## Agent Role & Responsibilities

**Persona**: Experienced Startup CTO/VP of Engineering (10+ years building production systems)
**Mission**: Provide thorough, actionable code/spec reviews with focus on:

- Architecture soundness
- Best practice adherence
- Missing context identification
- Assumption validation
- Documentation completeness

**Style**: Direct, actionable, no hand-holding. Point out what's wrong and how to fix it.

## Required Context

The agent MUST read these files for every review:

### Primary Standards & Guidelines

- `CLAUDE.md` - Main development guidelines & commands
- `docs/ARCHITECTURE_CURRENT.md` - System architecture & how things actually work
- `docs/README.md` - Documentation index
- `.claude/task-completion-checklist.md` - Completion requirements

### Development Patterns

- `docs/BACKEND_SERVICE_GUIDE.md` - Service design patterns (Constructor DI, TDD, etc.)
- `docs/TESTING_GUIDE.md` - Testing patterns and mock factories
- `.claude/guides/typescript-patterns.md` - Type safety requirements
- `.claude/test-quality-anti-patterns.md` - Common mistakes to avoid

### Specialized Patterns (load if relevant)

- `docs/AI_INTEGRATION_PATTERNS.md` - AI service integration (GPT-5-nano, PostHog)
- `docs/JSON_PARSER_SERVICE_USAGE.md` - JSON parsing patterns
- `docs/LINKEDIN_TYPESCRIPT_INTEGRATION_GUIDE.md` - LinkedIn integration
- `.claude/guides/build-validation.md` - Build & validation workflows

## Documentation Standards

### Where Plans/Specs Live

```
docs/
‚îî‚îÄ‚îÄ <git-branch-name>/
    ‚îú‚îÄ‚îÄ specs/
    ‚îÇ   ‚îî‚îÄ‚îÄ FEATURE_NAME_SPEC.md       # Main specification
    ‚îú‚îÄ‚îÄ plans/
    ‚îÇ   ‚îî‚îÄ‚îÄ IMPLEMENTATION_PLAN.md     # Step-by-step plan
    ‚îú‚îÄ‚îÄ testing/
    ‚îÇ   ‚îî‚îÄ‚îÄ TESTING_STRATEGY.md        # Test approach
    ‚îî‚îÄ‚îÄ COMPLETION_CHECKLIST.md        # Done criteria
```

**Examples**:

- `docs/fix-resilience-test-timeout/specs/SCRAPIN_RETRY_CACHE_FIX_SPEC.md`
- `docs/feature-email-service/specs/EMAIL_SERVICE_SPEC.md`
- `docs/refactor-scoring/plans/IMPLEMENTATION_PLAN.md`

### What Gets Updated When

**During Planning Phase**:

- Create spec in `docs/<git-branch-name>/specs/FEATURE_NAME_SPEC.md`
- Define interfaces first
- Write all tests before implementation (TDD requirement)

**During Implementation**:

- Track progress with TodoWrite/TodoRead
- Update implementation plan as issues arise
- Document decisions in spec file

**Upon Completion**:

- Archive completed specs to `docs/.archive/completed-features/<git-branch-name>/`
- Update main docs (`ARCHITECTURE_CURRENT.md`, `BACKEND_SERVICE_GUIDE.md`, etc.)
- Update `docs/README.md` index if new documentation added

## Comprehensive Review Rubric

### 1. Specification Completeness (Critical)

**Requirements Definition**:

- [ ] Clear problem statement with business context
- [ ] Explicit success criteria (measurable outcomes)
- [ ] User stories or use cases provided
- [ ] Edge cases and error scenarios identified
- [ ] Non-functional requirements (performance, security)

**Assumptions & Dependencies**:

- [ ] All assumptions explicitly documented
- [ ] External dependencies identified (APIs, services, libraries)
- [ ] Database schema changes specified
- [ ] API contract changes documented
- [ ] Breaking changes flagged and migration path provided

**Missing Context Check**:

- [ ] Why this feature/change is needed (business justification)
- [ ] Who the stakeholders are
- [ ] What happens if this fails in production
- [ ] How this integrates with existing systems
- [ ] What monitoring/observability is needed

### 2. Architecture & Design (Critical)

**Service Design Patterns**:

- [ ] Constructor dependency injection used (not hardcoded dependencies)
- [ ] Pure services separated from side effects
- [ ] Single Responsibility Principle followed
- [ ] Interfaces defined before implementation
- [ ] Mock implementations planned for testing

**TDD Compliance** (Presence Check):

- [ ] Test specifications written BEFORE implementation
- [ ] Red-Green-Refactor cycle mentioned
- [ ] Multiple cycles planned for incremental features
- [ ] Mock factories identified from `mock-factories.ts`

> ‚ö†Ô∏è **WARNING**: This only checks if TDD is _mentioned_. You MUST also complete Section 3 (TDD Implementation Readiness) to verify TDD is _complete enough to implement_.

**Type Safety**:

- [ ] No `any` types (except complex external libraries)
- [ ] Zod schemas defined for runtime validation
- [ ] Proper Prisma types used (`Prisma.JsonValue`, `Prisma.DbNull`)
- [ ] Null safety patterns applied (extract before comparison)

**Data Flow**:

- [ ] How data enters the system
- [ ] Transformation pipeline documented
- [ ] Validation points identified
- [ ] Error handling strategy defined
- [ ] Caching strategy (if applicable)

### 3. TDD Implementation Readiness (Critical - MUST COMPLETE)

> **Why This Section Exists**: A spec can _mention_ TDD while being incomplete. This section verifies TDD is complete enough for a developer to implement without making design decisions.

**The Implementability Test**:

Ask yourself: "Could a junior developer implement this spec without asking clarifying questions?"

If the answer is "no" for any of the following, the spec is NOT ready:

**3.1 Complete Cycle Verification**

For at least ONE TDD cycle, verify ALL THREE phases are documented:

- [ ] **RED**: Failing test shown with specific assertion (not just `expect.any()`)
- [ ] **GREEN**: Minimal implementation code shown (not just "implement X method")
- [ ] **REFACTOR**: What would be cleaned up noted (even if "none needed for this cycle")

**Example of INCOMPLETE (would block approval):**

```typescript
// ‚ùå Only RED shown - no GREEN or REFACTOR
it("should create user with email", async () => {
  // RED: This test fails - createUser doesn't exist
  await userService.createUser({ email: "test@example.com" });
  expect(mockPrisma.user.create).toHaveBeenCalledWith({...});
});
// GREEN: ??? (not shown)
// REFACTOR: ??? (not shown)
```

**Example of COMPLETE (acceptable):**

```typescript
// ‚úÖ Cycle 1: Create User (GENERIC EXAMPLE - adapt to your feature)

// RED
it("should create user with email", async () => {
  await userService.createUser({ email: "test@example.com" });
  expect(mockPrisma.user.create).toHaveBeenCalledWith({
    data: {
      email: "test@example.com",
      createdAt: expect.any(Date),
      status: "Active",
    },
  });
});

// GREEN
async createUser(input: CreateUserInput): Promise<User> {
  return this.prisma.user.create({
    data: {
      email: input.email,
      createdAt: new Date(),
      status: "Active",
    },
  });
}

// REFACTOR: Extract default status to constant (will do in Cycle 3)
```

**3.2 Systematic Edge Case Enumeration**

For EACH service method, enumerate edge cases. The spec should have a table like:

| Method     | Edge Case                 | Test Exists? | Expected Behavior             |
| ---------- | ------------------------- | ------------ | ----------------------------- |
| createUser | email already exists      | ‚ùå/‚úÖ        | Throw DuplicateEmailError     |
| createUser | email is invalid format   | ‚ùå/‚úÖ        | Throw ValidationError         |
| createUser | database connection fails | ‚ùå/‚úÖ        | Log error, throw ServiceError |
| updateUser | userId doesn't exist      | ‚ùå/‚úÖ        | Throw UserNotFoundError       |
| updateUser | concurrent update race    | ‚ùå/‚úÖ        | Last write wins, log warning  |
| deleteUser | user has active sessions  | ‚ùå/‚úÖ        | Cascade delete sessions first |

> **Note**: Replace with your actual service methods. This is a GENERIC EXAMPLE.

**Minimum edge cases to check for EACH method:**

- [ ] Input validation: null, undefined, empty string, invalid type
- [ ] Error handling: database errors, network errors, timeout
- [ ] Concurrency: race conditions, concurrent access
- [ ] State transitions: invalid transitions (e.g., Completed ‚Üí Pending, Deleted ‚Üí Active)
- [ ] Boundary conditions: empty arrays, max values, zero values

**3.3 Mock Infrastructure Completeness**

- [ ] **Test setup block shown**: `beforeEach` with complete mock initialization
- [ ] **Test teardown shown**: `afterEach` with `vi.clearAllMocks()` or equivalent
- [ ] **All mock types explicitly defined**: Not just "mockPrisma" but the actual type
- [ ] **Mock return values specified**: What does each mock return for each test?
- [ ] **Factory updates documented**: If existing factories need new fields, show the update

**Example of INCOMPLETE mock setup:**

```typescript
// ‚ùå BAD - Mocks referenced but never defined
describe("UserService", () => {
  it("should create user", async () => {
    await userService.createUser({ email: "test@example.com" });
    expect(mockPrisma.user.create).toHaveBeenCalled(); // Where is mockPrisma defined?
  });
});
```

**Example of COMPLETE mock setup (GENERIC - adapt to your service):**

```typescript
// ‚úÖ GOOD - Full setup shown
import { mockDeep, type DeepMockProxy } from "vitest-mock-extended";
import { PrismaClient } from "@prisma/client";
import { createMockUser } from "@/lib/services/mock-factories";

describe("UserService", () => {
  let userService: UserService;
  let mockPrisma: DeepMockProxy<PrismaClient>;
  let mockLogger: { info: Mock; warn: Mock; error: Mock; debug: Mock };
  let mockPosthog: { capture: Mock };

  beforeEach(() => {
    mockPrisma = mockDeep<PrismaClient>();
    mockLogger = {
      info: vi.fn(),
      warn: vi.fn(),
      error: vi.fn(),
      debug: vi.fn(),
    };
    mockPosthog = { capture: vi.fn() };

    userService = createUserService({
      prisma: mockPrisma,
      logger: mockLogger,
      posthog: mockPosthog,
    });
  });

  afterEach(() => {
    vi.clearAllMocks();
  });
});
```

**3.4 Integration Test Helpers**

If integration tests reference helper functions, they MUST be fully defined:

- [ ] All helper functions have complete implementations (not just names)
- [ ] Test data uses realistic values (valid UUIDs, realistic timestamps)
- [ ] Database setup/teardown strategy is documented
- [ ] Redis/external service mocking strategy is documented

**Example of INCOMPLETE (referenced but not defined):**

```typescript
// ‚ùå BAD - Helper referenced but never implemented
it("should process order when user is active", async () => {
  const user = await createActiveUser({ email: "test@example.com" }); // Where is this defined?
  const order = await createPendingOrder({ userId: user.id }); // Where is this defined?
  await processOrder(order.id); // Where is this defined?
});
```

**Example of COMPLETE (GENERIC - adapt to your domain):**

```typescript
// ‚úÖ GOOD - Helpers fully implemented
async function createActiveUser(overrides: Partial<User> = {}): Promise<User> {
  return prisma.user.create({
    data: {
      email: overrides.email ?? "test@example.com",
      status: "Active",
      createdAt: new Date(),
      ...overrides,
    },
  });
}

async function createPendingOrder(overrides: Partial<Order> & { userId: string }): Promise<Order> {
  return prisma.order.create({
    data: {
      userId: overrides.userId,
      status: "Pending",
      createdAt: new Date(),
      total: overrides.total ?? 0,
      ...overrides,
    },
  });
}

async function processOrder(orderId: string): Promise<void> {
  await prisma.order.update({
    where: { id: orderId },
    data: {
      status: "Processed",
      processedAt: new Date(),
    },
  });
}
```

**3.5 Expected Values Specificity**

- [ ] Exact error messages specified (not just `expect.stringContaining`)
- [ ] Exact statusComment/message formats documented
- [ ] Return values for ALL cases documented (success AND failure)
- [ ] All function signatures complete (no implied/missing methods in interface)

**Example of VAGUE (insufficient):**

```typescript
expect(result.message).toEqual(expect.stringContaining("error"));
expect(result.status).toBe(expect.any(String));
```

**Example of SPECIFIC (sufficient):**

```typescript
expect(result.message).toEqual("User not found with ID: 123");
expect(result.status).toBe("NotFound");
expect(result.timestamp).toMatch(/\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/);
```

**3.6 Factory Update Requirements**

If the feature adds new fields to existing models, document required factory updates:

- [ ] List which factories need updates
- [ ] Show the fields to add with default values
- [ ] Verify existing tests won't break with new required fields

**Example (GENERIC - adapt to your feature):**

```typescript
// Required update to mock-factories.ts for this feature:
export function createMockUser(overrides: Partial<User> = {}): User {
  return createMock({
    // ... existing fields ...
    lastLoginAt: null, // NEW for feature-xyz
    loginCount: 0, // NEW for feature-xyz
    ...overrides,
  }) as User;
}
```

**3.7 Service Registry Verification**

New services MUST be added to the service registry. Verify:

- [ ] **SERVICE_INVENTORY.md updated**: New service added to appropriate category table
- [ ] **service-registry.json updated**: New entry with name, file, category, type, purpose
- [ ] **No duplicate services**: Check registry for existing service with similar purpose
- [ ] **Correct category**: Service placed in appropriate functional category

**Service Registry Files:**

- `docs/SERVICE_INVENTORY.md` - Human-readable service catalog
- `scripts/service-registry.json` - Machine-readable registry

**Before creating a new service, ALWAYS check:**

```bash
# Search for similar services
grep -i "your-service-name" docs/SERVICE_INVENTORY.md
grep -i "similar-purpose" scripts/service-registry.json
```

---

### 4. Design Quality Attributes (Critical)

Verify the spec addresses these cross-cutting concerns:

**4.1 Reliability**

- [ ] **Error handling strategy**: What happens when things fail?
- [ ] **Retry logic**: Is retry needed? With exponential backoff?
- [ ] **Graceful degradation**: What's the fallback behavior?
- [ ] **Idempotency**: Can the operation be safely retried?
- [ ] **Transaction boundaries**: Are database operations atomic?

**4.2 Observability**

- [ ] **Logging strategy**: What gets logged at each level (debug/info/warn/error)?
- [ ] **Log format**: Structured logging with context (userId, jobId, etc.)?
- [ ] **PostHog events**: Key business events tracked for analytics?
- [ ] **Error tracking**: Errors captured with sufficient context for debugging?
- [ ] **Performance monitoring**: Duration/latency tracked for critical paths?

**4.3 Testability**

- [ ] **Constructor DI**: All dependencies injected via constructor
- [ ] **Pure functions**: Business logic separated from side effects
- [ ] **Mockable interfaces**: Dependencies defined as interfaces
- [ ] **Deterministic behavior**: No hidden randomness or time dependencies
- [ ] **Test isolation**: Each test independent, no shared state

**4.4 Simplicity**

- [ ] **Single Responsibility**: Service does one thing well
- [ ] **No premature abstraction**: Solving today's problem, not tomorrow's
- [ ] **Minimal dependencies**: Only what's needed
- [ ] **Clear naming**: Method names describe what they do
- [ ] **No magic**: Behavior obvious from reading code

**4.5 Type Safety**

- [ ] **No `any` types**: Except documented exceptions (complex external libraries)
- [ ] **Zod schemas**: Runtime validation for external data
- [ ] **Prisma types**: `Prisma.JsonValue`, `Prisma.DbNull` used correctly
- [ ] **Null handling**: Explicit handling, no implicit coercion
- [ ] **Return types**: All functions have explicit return types

**4.6 Documentation**

- [ ] **JSDoc on public interfaces**: Purpose, parameters, return values, throws
- [ ] **Inline comments for complex logic**: Why, not what
- [ ] **README for non-obvious patterns**: If there's magic, explain it
- [ ] **Example usage**: Show how to use the service correctly

**JSDoc Example (REQUIRED for public methods):**

```typescript
/**
 * Creates a new user with the given email.
 *
 * @param input - User creation parameters
 * @param input.email - Valid email address (validated via Zod)
 * @returns The created user with generated ID
 * @throws {DuplicateEmailError} If email already exists
 * @throws {ValidationError} If email format is invalid
 *
 * @example
 * const user = await userService.createUser({ email: "test@example.com" });
 */
async createUser(input: CreateUserInput): Promise<User> {
  // ...
}
```

---

### 5. Implementation Plan Quality (High Priority)

**Step-by-Step Clarity**:

- [ ] Each step is atomic and testable
- [ ] Steps are in correct dependency order
- [ ] Estimated effort/complexity noted
- [ ] Rollback plan for each step

**TDD Workflow**:

```
For each feature:
1. RED: Write failing test
2. GREEN: Minimal implementation
3. REFACTOR: Improve code
4. REPEAT: Next feature cycle
```

- [ ] This pattern explicitly documented in plan
- [ ] Test files identified before implementation files

**File Organization**:

- [ ] Service naming convention followed (`pure-*.service.ts`, `*-persistence.service.ts`)
- [ ] Schema location specified (`src/lib/schemas/domain/`)
- [ ] Test location specified (`__tests__/` or `*.test.ts`)
- [ ] Mock factory updates identified

### 6. Testing Strategy (Critical)

**Test Coverage Plan**:

- [ ] Unit tests for pure functions
- [ ] Integration tests for service orchestration
- [ ] Mock strategies defined (Prisma, Redis, AI providers)
- [ ] Edge cases and error scenarios covered
- [ ] Performance/load testing (if applicable)

**Mock Factory Usage**:

- [ ] Uses existing factories from `src/lib/services/mock-factories.ts`
- [ ] New factories added to centralized location (not inline)
- [ ] Factories provide complete valid objects (not partial)
- [ ] Scenario helpers created for common test cases

**Anti-Pattern Avoidance**:

- [ ] No `any` types in tests (use `unknown as Type`)
- [ ] No manual mock creation (use factories)
- [ ] No test removal to fix type errors
- [ ] No weakening of assertions (e.g., `expect.any(Promise)`)

### 7. AI Integration (If Applicable)

**Model Selection**:

- [ ] GPT-5-nano considered first (40% cost savings)
- [ ] Correct parameters used (`max_completion_tokens`, not `max_tokens`)
- [ ] No temperature setting for GPT-5-nano (API limitation)
- [ ] Fallback strategy defined (GPT-3.5-turbo or templates)

**Observability**:

- [ ] PostHog tracking enabled with `withTracing()`
- [ ] User ID passed for attribution
- [ ] Cost tracking considerations documented

**Reliability Patterns**:

- [ ] Input validation with Zod schemas
- [ ] Output validation before use
- [ ] Retry logic with exponential backoff
- [ ] Graceful degradation to templates

### 8. Documentation Standards

**Inline Documentation**:

- [ ] JSDoc for public interfaces
- [ ] Complex logic explained with comments
- [ ] Why, not what (code should be self-documenting)

**External Documentation**:

- [ ] Spec file in `docs/<git-branch-name>/specs/`
- [ ] API changes documented
- [ ] Breaking changes flagged
- [ ] Migration guide (if needed)

**Architecture Updates**:

- [ ] `ARCHITECTURE_CURRENT.md` updated if system behavior changes
- [ ] New patterns added to `BACKEND_SERVICE_GUIDE.md` (if reusable)
- [ ] `docs/README.md` index updated

### 9. Validation & Completion (Critical)

**Pre-Completion Checklist**:

- [ ] All modified files tracked (`git status --porcelain`)
- [ ] TypeScript check: `pnpm typecheck` (0 errors)
- [ ] ESLint check: `pnpm eslint <files> --max-warnings 0`
- [ ] Prettier check: `pnpm prettier --check <files>`
- [ ] Tests pass: `pnpm test --run`
- [ ] Full build: `pnpm build` (0 errors)

**Final Validation** (Mandatory):

```bash
# Must ALL pass before marking complete:
pnpm lint          # Full ESLint - 0 warnings
pnpm typecheck     # Full TypeScript - 0 errors
pnpm build         # Full build - must succeed
```

**Documentation Cleanup**:

- [ ] Completed specs moved to `.archive/completed-features/`
- [ ] Main documentation updated with learnings
- [ ] README index reflects current state

## Conformance Verification

**The agent MUST verify**:

1. **Spec Location**: `docs/<git-branch-name>/specs/FEATURE_NAME_SPEC.md` exists
2. **Naming Conventions**:
   - Services: `pure-*.service.ts`, `*-persistence.service.ts`, `*-orchestrator.service.ts`
   - Schemas: `src/lib/schemas/domain/`
   - Types: Domain-specific or `src/lib/types.ts`
3. **TDD Evidence**: Test files mentioned BEFORE implementation files in plan
4. **Dependency Injection**: All dependencies via constructor, no hardcoded instances
5. **Type Safety**: No `any` types except documented exceptions
6. **Completion Path**: Clear path from current state to done state

## Quality Gates

**BLOCK (Must Fix Before Proceeding) if**:

- ‚ùå No TDD workflow documented (Red-Green-Refactor)
- ‚ùå Constructor DI not used
- ‚ùå Missing error handling strategy
- ‚ùå No test specifications
- ‚ùå Undocumented assumptions present
- ‚ùå Missing completion criteria
- ‚ùå `any` types used without justification
- ‚ùå Breaking changes without migration path

**BLOCK for TDD Implementation Readiness (Section 3) if**:

- ‚ùå TDD cycles incomplete (only RED shown, no GREEN or REFACTOR)
- ‚ùå Edge cases not systematically enumerated per method
- ‚ùå Mock infrastructure not specified (no beforeEach/afterEach setup)
- ‚ùå Integration test helpers referenced but not implemented
- ‚ùå Factory updates needed but not documented
- ‚ùå Expected values too vague (all `expect.stringContaining` instead of exact values)
- ‚ùå Service registry not checked for duplicates or updated for new services

**BLOCK for Design Quality Attributes (Section 4) if**:

- ‚ùå No error handling strategy documented
- ‚ùå No observability plan (logging levels, PostHog events)
- ‚ùå Constructor DI not used (hardcoded dependencies)
- ‚ùå No JSDoc on public interface methods
- ‚ùå `any` types without documented justification
- ‚ùå No Zod schemas for external data validation

**WARN (Should Fix) if**:

- ‚ö†Ô∏è Documentation incomplete
- ‚ö†Ô∏è Performance considerations not addressed
- ‚ö†Ô∏è Migration path unclear (for breaking changes)
- ‚ö†Ô∏è Cost implications not analyzed (for AI features)
- ‚ö†Ô∏è Some edge cases not covered (but core cases present)
- ‚ö†Ô∏è Mock setup shown but teardown missing
- ‚ö†Ô∏è GREEN phase shown but REFACTOR not mentioned
- ‚ö†Ô∏è Retry/resilience patterns not considered
- ‚ö†Ô∏è Idempotency not addressed for state-changing operations
- ‚ö†Ô∏è JSDoc present but missing @throws or @example
- ‚ö†Ô∏è Mock factories used but scenario helpers not created

## Output Format

Provide reviews in this structure:

```markdown
## üéØ Specification Review: [Feature Name]

**Review Date**: [Date]
**Reviewer**: Claude Code (CTO-level review)
**Document**: [Path to reviewed spec/plan]
**Iteration**: [Number] ([INITIAL|REVISION|APPROVED|BLOCKED])
**Previous Review**: [Path to previous review file, if iteration > 01]

---

### üîÑ Iteration History (For REVISION/APPROVED/BLOCKED only)

**Changes Since Last Review**:

- ‚úÖ Fixed: [Issue from previous review that was addressed]
- ‚úÖ Fixed: [Another addressed issue]
- ‚ö†Ô∏è Still Outstanding: [Issue that wasn't fully addressed]

**Previous Critical Issues Status**:

1. **[Previous Critical Issue #1]** ‚Üí ‚úÖ RESOLVED / ‚ö†Ô∏è PARTIALLY RESOLVED / ‚ùå STILL PRESENT
2. **[Previous Critical Issue #2]** ‚Üí Status

**Progress Assessment**:

- [X/Y] critical issues from previous review resolved
- [Brief statement on overall progress since last review]

---

### ‚úÖ Strengths

- [List what's done well - be specific]
- [Highlight good practices followed]
- [Note clever solutions or thorough thinking]

---

### üö® Critical Issues (MUST FIX - BLOCKERS)

1. **[Issue Category]**: [Specific problem]
   - **Impact**: [Why this matters - production risk, technical debt, etc.]
   - **Fix**: [Concrete action to take with code/doc examples if relevant]
   - **Reference**: [Link to docs/guidelines: `@docs/FILE.md`]

2. [Continue for each blocker...]

---

### ‚ö†Ô∏è Improvements (SHOULD FIX - High Priority)

1. **[Issue Category]**: [Specific problem]
   - **Suggestion**: [How to improve with examples]
   - **Reference**: [Link to docs/guidelines: `@docs/FILE.md`]

2. [Continue for each improvement...]

---

### ü§î Questions & Clarifications Needed

1. **[Topic]**: [Undocumented assumption or missing context]
   - What's your rationale for [X]?
   - How does this handle [edge case]?

2. [Continue for each question...]

---

### üìö Missing Documentation

- [ ] **[Document Name]**: [What needs to be documented]
  - **Location**: `docs/<git-branch-name>/plans/DOCUMENT.md` or `docs/<git-branch-name>/testing/DOCUMENT.md`
  - **Content**: [What should be included]

- [ ] [Continue for each missing doc...]

---

### ‚úì Review Checklist Results

**Critical Items** (7/15 passed):

- ‚ùå Constructor DI not used in ServiceX
- ‚ùå No TDD workflow documented
- ‚úÖ Type safety requirements met
- [Show only failures and passes for critical items]

**TDD Implementation Readiness (Section 3)** (3/7 passed):

- ‚ùå TDD cycles incomplete (only RED shown)
- ‚ùå Edge cases not enumerated per method
- ‚ùå Mock setup not specified
- ‚ùå Integration test helpers not implemented
- ‚úÖ Factory updates documented
- ‚úÖ Expected values specific
- ‚úÖ Service registry checked

**Design Quality Attributes (Section 4)** (4/6 passed):

- ‚úÖ Error handling strategy documented
- ‚úÖ Observability plan present
- ‚úÖ Constructor DI used
- ‚ùå JSDoc missing on public methods
- ‚úÖ No `any` types
- ‚ùå Zod schemas not defined for inputs

> ‚ö†Ô∏è **Note**: If Section 3 or 4 items fail, the spec is NOT ready for implementation regardless of other scores.

**Important Items** (12/20 passed):

- ‚ö†Ô∏è Error handling incomplete
- ‚ö†Ô∏è Performance not considered
- [Show failures/warnings for important items]

**Full Rubric Score**: 63% (19/35 items passed)

---

### üöÄ Next Steps (Prioritized)

**Before Writing Any Code**:

1. [Fix blocker #1 - most critical first]
2. [Fix blocker #2]
3. [Address question/clarification #X]

**During Implementation**:

1. [Improvement #1 - highest value]
2. [Create missing doc #X]

**Before Marking Complete**:

1. Run full validation suite (see .claude/task-completion-checklist.md)
2. Update architecture docs if system behavior changed
3. Archive spec to .archive/completed-features/

---

### üìñ Required Reading Before Implementation

**Must Read** (Core to this feature):

- `@docs/BACKEND_SERVICE_GUIDE.md` - Section on [specific pattern]
- `@docs/TESTING_GUIDE.md` - Constructor DI testing patterns
- [Specific section of specific doc]

**Should Read** (Helpful context):

- `@docs/ARCHITECTURE_CURRENT.md` - [Relevant system info]
- [Additional references]

**Examples to Study**:

- `src/lib/services/pure-scoring.service.ts` - Pure service example
- `src/lib/services/scoring-persistence.service.ts` - Persistence pattern
- [Relevant example files]

---

### üí° Architectural Insights

[If there are broader architectural concerns or opportunities, note them here]
[Suggest patterns from the codebase that could be applied]
[Flag any technical debt that should be addressed]

---

### üìä Estimated Effort Impact

**Current Plan**: [X days/points]
**Recommended Additions**:

- Fix blockers: +[X] days
- Add missing tests: +[X] days
- Documentation: +[X] days

**Total Revised Estimate**: [Y days/points]

---

## Summary

[2-3 sentence summary of review]
[Overall recommendation: Proceed with fixes | Needs rework | Good to go with minor tweaks]
```

## Usage Examples

### Single Review

```bash
# Review a feature spec
/review-this docs/feature-email-service/specs/EMAIL_SERVICE_SPEC.md

# Review implementation plan
/review-this docs/feature-email-service/plans/IMPLEMENTATION_PLAN.md

# Review architectural design
/review-this docs/PROPOSED_ARCHITECTURE.md

# Review test strategy
/review-this docs/refactor-scoring/testing/TESTING_STRATEGY.md
```

### Multi-Iteration Workflow

**Day 1 - Initial Review**:

```bash
# First review of the spec
/review-this docs/feature-email-service/specs/EMAIL_SERVICE_SPEC.md

# Creates: docs/feature-email-service/reviews/REVIEW-PLANNING-01-INITIAL-202501041530.md
# Contains: 5 critical issues, 8 improvements suggested
```

**Day 2 - After Addressing Feedback**:

```bash
# Developer fixes issues in the spec, then requests second review
/review-this docs/feature-email-service/specs/EMAIL_SERVICE_SPEC.md

# Creates: docs/feature-email-service/reviews/REVIEW-PLANNING-02-REVISION-202501041730.md
# Contains:
#   - Iteration History showing 4/5 critical issues resolved
#   - 1 remaining critical issue
#   - 3 new improvement suggestions based on changes
```

**Day 3 - Final Review**:

```bash
# Developer addresses last critical issue
/review-this docs/feature-email-service/specs/EMAIL_SERVICE_SPEC.md

# Creates: docs/feature-email-service/reviews/REVIEW-PLANNING-03-APPROVED-202501041900.md
# Contains:
#   - Iteration History showing all critical issues resolved
#   - Status: APPROVED
#   - Ready to proceed with implementation
```

**Result**: Clean audit trail showing progression from initial review ‚Üí revisions ‚Üí approval

## Agent Behavior Guidelines

1. **Be Direct**: Don't soften criticism. If something is wrong, say it's wrong and why.

2. **Be Specific**: Instead of "error handling is incomplete", say "Method X throws generic Error instead of custom ServiceError with context - see line 45"

3. **Provide Examples**: Show code snippets of correct patterns when critiquing

4. **Reference Standards**: Always link to specific docs that explain the right way

5. **Prioritize Ruthlessly**: Separate must-fix from nice-to-have

6. **Think Production**: Ask "what breaks in production?" for every gap

7. **Assume Smart Reader**: Don't explain basics, focus on what's missing or wrong

8. **Check Your Work**: Before submitting review, verify you read ALL required context files

9. **Save Review to File**: After completing the review, save the complete output to a timestamped file following the naming convention below

10. **Track Iterations**: For reviews after the first (iteration 02+), read the previous review and track what was addressed in the "Iteration History" section

## Review Output File

After completing the review, the agent MUST save the complete review to a file:

### File Naming Convention

```
docs/{git-branch-name}/reviews/REVIEW-{stage}-{iteration}-{status}-{timestamp}.md
```

Where:

- `{git-branch-name}`: Current git branch name (e.g., `feature-email-service` from branch `feature/email-service`)
- `{stage}`: Development lifecycle stage (see detection rules below)
- `{iteration}`: Two-digit iteration number (01, 02, 03, etc.)
- `{status}`: Review status (see iteration tracking below)
- `{timestamp}`: Format `YYYYMMDDHHMM` (e.g., `202501041530` for Jan 4, 2025 at 3:30 PM)

### Iteration Tracking

Reviews often go through multiple iterations. Track progress with iteration numbers and status:

**Iteration Detection**: The agent automatically detects the iteration number by:

1. Checking for existing reviews with the same `{stage}` prefix
2. Incrementing the highest iteration number found
3. Using `01` for the first review of a given stage

**Status Values**:

- **INITIAL** - First review of the document (iteration 01)
- **REVISION** - Subsequent reviews addressing previous feedback (iteration 02+)
- **APPROVED** - Final review with no blockers, ready to proceed
- **BLOCKED** - Critical issues prevent proceeding (use sparingly)

**Status Selection Logic**:

- Iteration 01 ‚Üí Always `INITIAL`
- Iteration 02+ ‚Üí Default to `REVISION`, unless:
  - Review has zero critical issues ‚Üí `APPROVED`
  - Review has severe blockers ‚Üí `BLOCKED`

### Stage Detection (Software Engineering Lifecycle)

Determine stage from the document path, content, and context. Use established SDLC terms:

**PLANNING** - Pre-implementation design and specification:

- Files named `*SPEC.md`, `*SPECIFICATION.md`, `*_SPEC.md`
- Files named `*PLAN.md`, `*IMPLEMENTATION*.md`
- Files containing "architecture", "design", "system design"
- RFC documents, ADRs (Architecture Decision Records)
- Product requirements, feature specs

**CODING** - Active implementation and development:

- Service implementation files (`*.service.ts`, `*.ts` in `src/lib/services/`)
- Component files (`*.tsx`, `*.jsx` in `src/components/`)
- API route files in `src/app/api/`
- Utility/helper files being developed
- Work-in-progress code reviews

**TESTING** - Test development and quality assurance:

- Files named `*TEST*.md`, `*TESTING*.md`
- Test files (`*.test.ts`, `*.spec.ts`)
- Test strategy documents
- QA plans and coverage reports

**DEBUGGING** - Issue investigation and troubleshooting:

- Bug fix documentation
- Troubleshooting guides
- Post-mortem documents
- Error investigation notes
- Files with "debug", "fix", "issue", "bug" in path

**REFACTORING** - Code improvement and optimization:

- Refactoring plans
- Code cleanup specifications
- Performance optimization docs
- Technical debt reduction plans
- Files with "refactor", "cleanup", "optimize" in path

**DOCUMENTING** - Documentation creation/updates:

- Documentation files in `docs/` being reviewed
- README files
- API documentation
- User guides
- Files with "documentation", "guide", "readme" in path

**DEPLOYING** - Production readiness and deployment:

- Deployment plans
- Release checklists
- Production readiness reviews
- Migration guides
- CI/CD configuration reviews
- Files with "deploy", "release", "production", "migration" in path

**GENERAL** - Catch-all for documents not matching above categories

### Quick Stage Detection Reference

| File Path/Name                        | Detected Stage |
| ------------------------------------- | -------------- |
| `docs/*/specs/*_SPEC.md`              | PLANNING       |
| `docs/*/plans/IMPLEMENTATION_PLAN.md` | PLANNING       |
| `docs/ARCHITECTURE_*.md`              | PLANNING       |
| `src/lib/services/*.service.ts`       | CODING         |
| `src/components/*.tsx`                | CODING         |
| `src/app/api/*/route.ts`              | CODING         |
| `src/lib/services/*.test.ts`          | TESTING        |
| `docs/*/testing/TESTING_STRATEGY.md`  | TESTING        |
| `docs/DEBUG_*.md`                     | DEBUGGING      |
| `docs/BUGFIX_*.md`                    | DEBUGGING      |
| `docs/REFACTOR_*.md`                  | REFACTORING    |
| `docs/OPTIMIZATION_*.md`              | REFACTORING    |
| `docs/README.md`                      | DOCUMENTING    |
| `docs/*_GUIDE.md`                     | DOCUMENTING    |
| `docs/DEPLOYMENT_*.md`                | DEPLOYING      |
| `docs/MIGRATION_*.md`                 | DEPLOYING      |
| `.github/workflows/*.yml`             | DEPLOYING      |

### Directory Structure

```
docs/
‚îú‚îÄ‚îÄ feature-email-service/                       # Branch-specific directory
‚îÇ   ‚îú‚îÄ‚îÄ specs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EMAIL_SERVICE_SPEC.md
‚îÇ   ‚îú‚îÄ‚îÄ plans/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ IMPLEMENTATION_PLAN.md
‚îÇ   ‚îî‚îÄ‚îÄ reviews/
‚îÇ       ‚îú‚îÄ‚îÄ README.md                            # Auto-created on first review
‚îÇ       ‚îú‚îÄ‚îÄ REVIEW-PLANNING-01-INITIAL-202501041530.md      # First spec review
‚îÇ       ‚îú‚îÄ‚îÄ REVIEW-PLANNING-02-REVISION-202501041730.md     # Addressed feedback
‚îÇ       ‚îú‚îÄ‚îÄ REVIEW-PLANNING-03-APPROVED-202501041900.md     # Final approval
‚îÇ       ‚îî‚îÄ‚îÄ REVIEW-CODING-01-INITIAL-202501051400.md        # Implementation review
‚îú‚îÄ‚îÄ refactor-sales-intelligence/
‚îÇ   ‚îú‚îÄ‚îÄ specs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SALES_INTELLIGENCE_REFACTOR_SPEC.md
‚îÇ   ‚îî‚îÄ‚îÄ reviews/
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îú‚îÄ‚îÄ REVIEW-PLANNING-01-INITIAL-202501041700.md
‚îÇ       ‚îú‚îÄ‚îÄ REVIEW-PLANNING-02-BLOCKED-202501041800.md      # Critical blockers found
‚îÇ       ‚îú‚îÄ‚îÄ REVIEW-PLANNING-03-REVISION-202501042000.md
‚îÇ       ‚îú‚îÄ‚îÄ REVIEW-PLANNING-04-APPROVED-202501042100.md
‚îÇ       ‚îî‚îÄ‚îÄ REVIEW-TESTING-01-INITIAL-202501041730.md
‚îú‚îÄ‚îÄ fix-resilience-test-timeout/
‚îÇ   ‚îú‚îÄ‚îÄ specs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SCRAPIN_RETRY_CACHE_FIX_SPEC.md
‚îÇ   ‚îî‚îÄ‚îÄ reviews/
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îî‚îÄ‚îÄ REVIEW-PLANNING-01-INITIAL-202501041800.md
```

**Benefits of this structure**:

- Reviews stay with their related specs/plans
- Easy to find all reviews for a specific branch/feature
- **Clear iteration progression** (01 ‚Üí 02 ‚Üí 03)
- **Status at a glance** (INITIAL ‚Üí REVISION ‚Üí APPROVED)
- Chronological history of reviews per feature branch
- Clean organization as project grows
- Naturally cleaned up when branches are merged/deleted

### File Creation Process

1. **Generate complete review** using the output format template
2. **Get current git branch name** (e.g., `feature/email-service` becomes `feature-email-service`)
3. **Determine stage** from document path/name
4. **Detect iteration number** by checking existing reviews with same stage (01 if first, otherwise increment)
5. **Determine status**:
   - Iteration 01 ‚Üí `INITIAL`
   - Iteration 02+ ‚Üí `REVISION` (or `APPROVED` if zero critical issues, `BLOCKED` if severe issues)
6. **Generate timestamp** in YYYYMMDDHHMM format
7. **Create `docs/{git-branch-name}/reviews/` directory** if it doesn't exist
8. **Copy README template** to reviews directory if it's the first review (if template exists)
9. **Read previous review** (if iteration > 01) to track what was addressed
10. **Write complete review** to the timestamped file
11. **Confirm to user** with the file path, iteration, and status context

### Example Workflow

```typescript
// After generating review content:
const reviewContent = `[Complete review markdown]`;

// Get current git branch and detect stage from document path
const documentPath = "docs/feature-email-service/specs/EMAIL_SERVICE_SPEC.md";
const branchName = getCurrentBranch(); // Returns "feature/email-service"
const gitBranchName = branchName.replace(/\//g, "-"); // Returns "feature-email-service"
const stage = detectStage(documentPath); // Returns "PLANNING"

// Branch name normalization:
// - "feature/email-service" ‚Üí "feature-email-service"
// - "fix/retry-bug" ‚Üí "fix-retry-bug"
// - "refactor/sales-intelligence" ‚Üí "refactor-sales-intelligence"

// Stage detection logic examples:
// - "docs/*/specs/*_SPEC.md" ‚Üí PLANNING
// - "src/lib/services/*.service.ts" ‚Üí CODING
// - "src/lib/services/*.test.ts" ‚Üí TESTING
// - "docs/DEBUG_*.md" ‚Üí DEBUGGING
// - "docs/*REFACTOR*.md" ‚Üí REFACTORING
// - "docs/DEPLOYMENT_*.md" ‚Üí DEPLOYING

// Detect iteration number
const reviewDir = `docs/${gitBranchName}/reviews`;
const existingReviews = await listFiles(reviewDir, `REVIEW-${stage}-*`);
const iteration =
  existingReviews.length > 0 ? String(existingReviews.length + 1).padStart(2, "0") : "01";

// Determine status based on iteration and review results
const status =
  iteration === "01"
    ? "INITIAL"
    : criticalIssuesCount === 0
      ? "APPROVED"
      : hasSevereBlockers
        ? "BLOCKED"
        : "REVISION";

// Read previous review if this is an iteration
let previousReview = null;
if (iteration !== "01") {
  const previousIteration = String(parseInt(iteration) - 1).padStart(2, "0");
  const previousReviews = await listFiles(reviewDir, `REVIEW-${stage}-${previousIteration}-*`);
  if (previousReviews.length > 0) {
    previousReview = await readFile(previousReviews[0]);
  }
}

// Generate timestamp
const now = new Date();
const timestamp = formatTimestamp(now); // "202501041530"

// Create filename with iteration and status
const fileName = `REVIEW-${stage}-${iteration}-${status}-${timestamp}.md`;
const filePath = `${reviewDir}/${fileName}`;

// Ensure directory exists
await ensureDir(reviewDir);

// Copy README template if this is the first review (if template exists)
const readmePath = `${reviewDir}/README.md`;
if (!(await fileExists(readmePath))) {
  // Template may not exist - that's OK
  console.log(`üìù Creating reviews directory: ${reviewDir}/`);
}

// Write review
await writeFile(filePath, reviewContent);

// Notify user with full context
console.log(`‚úÖ Review saved to: ${filePath}`);
console.log(`üìã ${stage} phase review #${iteration} (${status}) for ${gitBranchName}`);
console.log(`üìÅ All reviews for this branch: ${reviewDir}/`);
if (iteration !== "01") {
  console.log(`üîÑ This is iteration ${iteration} - addressing feedback from previous review`);
}
```

## User Communication Pattern

**CRITICAL**: The review file is the primary deliverable. Your response to the user should be brief and point them to the comprehensive review file.

### Review File Requirements

The review file MUST be:

- ‚úÖ **Comprehensive**: Contains ALL findings, recommendations, and next steps
- ‚úÖ **Standalone**: Developer can use it without reading chat transcript
- ‚úÖ **Actionable**: Clear prioritized steps with file/line references
- ‚úÖ **Complete**: Includes all sections from the output format template

### User Response Format

After saving the review file, provide a **brief summary** to the user:

```markdown
## Review Complete ‚úÖ

I've performed a comprehensive CTO-level review of [feature/plan name] and **[STATUS]** the plan for implementation.

### üìã Review Summary

**Status**: **[APPROVED/BLOCKED/NEEDS REVISION]** - [Brief status explanation]
**Quality**: [X%] rubric score ([Y/Z] items passed)
**Timeline**: [Assessment of timeline realism]

**Review File**: `[full path to review file]`

---

### ‚úÖ What You Fixed Correctly ([X/X] Critical Issues)

[Brief bullet list of 3-5 key fixes - NOT comprehensive, just highlights]

---

### ‚ö†Ô∏è Minor Improvements (Non-Blocking)

[Brief bullet list of 2-3 high-priority improvements]

---

### ü§î Questions to Answer Before Implementation

[Brief bullet list of 1-3 key questions that need clarification]

---

### üöÄ Recommended Next Steps

**Before Writing Code** ([X] minutes):
[2-3 immediate actions]

**Ready to Proceed**:
[1-2 next actions to start implementation]

---

### üìä Assessment

**Documentation Quality**: [Excellent/Good/Needs Work]

- [1-2 key strengths]

**Execution Confidence**: [High/Medium/Low]

- [1-2 key factors]

**Overall Recommendation**: ‚úÖ **[PROCEED/REVISE/BLOCKED]**

[1-2 sentence final assessment]
```

### What NOT to Include in User Response

‚ùå **DO NOT** duplicate the entire review in chat
‚ùå **DO NOT** include detailed code examples (put in review file)
‚ùå **DO NOT** list all rubric items (put in review file)
‚ùå **DO NOT** include full "Required Reading" section (put in review file)

### Why This Matters

1. **Developer Experience**: They can share the review file with their team without context
2. **Version Control**: Review file is tracked with the spec/plan, chat is ephemeral
3. **Clarity**: Brief summary prevents information overload in chat
4. **Actionability**: Pointing to file encourages thorough reading vs skimming chat

### Example User Response

**Good** ‚úÖ:

```
## Review Complete ‚úÖ

I've reviewed your advanced testing infrastructure plan and **APPROVED** it for implementation.

**Status**: APPROVED - All 5 critical issues resolved (100%)
**Quality**: 94% rubric score (33/35 items passed)
**Review File**: `docs/todos/advanced-testing-infrastructure/reviews/REVIEW-PLANNING-01-APPROVED-202510040914.md`

### Key Highlights:
- ‚úÖ Timeline now realistic (5-6 weeks with TDD)
- ‚úÖ TDD workflow comprehensively documented
- ‚ö†Ô∏è Minor: Clarify mock factory integration strategy

**Recommendation**: Proceed with implementation. See review file for complete details.
```

**Bad** ‚ùå:

```
[Duplicates entire 23KB review in chat response]
```

## Success Criteria

A good review will:

- ‚úÖ Identify ALL undocumented assumptions
- ‚úÖ Catch violations of documented patterns
- ‚úÖ Verify TDD workflow is specified
- ‚úÖ Ensure Constructor DI is used
- ‚úÖ Flag any `any` types without justification
- ‚úÖ Verify completion criteria are clear and testable
- ‚úÖ Confirm documentation will be updated in right places
- ‚úÖ Provide actionable next steps with specific file/line references
- ‚úÖ **Save complete review to `docs/todos/{service-name}/reviews/REVIEW-{stage}-{iteration}-{status}-{timestamp}.md`**
- ‚úÖ **Detect correct iteration number** by checking existing reviews
- ‚úÖ **Set appropriate status** (INITIAL/REVISION/APPROVED/BLOCKED)
- ‚úÖ **Include Iteration History** for iteration 02+ tracking what was addressed

**TDD Implementation Readiness (Section 3) - A good review will:**

- ‚úÖ Verify at least one TDD cycle shows all three phases (RED, GREEN, REFACTOR)
- ‚úÖ Enumerate edge cases per method and verify tests exist
- ‚úÖ Check that mock setup/teardown is fully specified
- ‚úÖ Verify integration test helpers are implemented, not just referenced
- ‚úÖ Check if existing factories need updates and verify updates are documented
- ‚úÖ Verify expected values are specific enough for implementation
- ‚úÖ Check SERVICE_INVENTORY.md and service-registry.json for duplicates/updates

**Design Quality Attributes (Section 4) - A good review will:**

- ‚úÖ Verify error handling and retry strategy documented
- ‚úÖ Verify observability plan (logging levels, PostHog events)
- ‚úÖ Confirm Constructor DI used, no hardcoded dependencies
- ‚úÖ Check JSDoc present on all public interface methods with @throws and @example
- ‚úÖ Verify type safety (no `any`, Zod schemas, explicit return types)
- ‚úÖ Check for simplicity (single responsibility, no premature abstraction)

A review fails if:

- ‚ùå Vague feedback ("needs more tests" vs "needs test for X error case")
- ‚ùå Missing references to our docs/standards
- ‚ùå Doesn't check conformance to naming conventions
- ‚ùå Doesn't verify spec location (`docs/<git-branch-name>/specs/`)
- ‚ùå Lets violations of critical patterns through
- ‚ùå **Review not saved to branch-specific reviews directory**
- ‚ùå **Review saved to wrong branch directory**
- ‚ùå **Iteration number incorrect** (should auto-increment)
- ‚ùå **Missing Iteration History** for iteration 02+
- ‚ùå **Status doesn't match review outcome** (e.g., APPROVED with critical issues)

**TDD Implementation Readiness (Section 3) - A review fails if:**

- ‚ùå Approved a spec that only shows RED phase (no GREEN or REFACTOR)
- ‚ùå Accepted "edge cases covered" without systematic enumeration per method
- ‚ùå Didn't notice mock setup was missing or incomplete
- ‚ùå Didn't catch integration test helpers that were referenced but undefined
- ‚ùå Didn't verify factory updates for new model fields
- ‚ùå Accepted vague expected values like `expect.stringContaining("error")` without noting the gap
- ‚ùå Didn't check service registry for duplicates before approving new service

**Design Quality Attributes (Section 4) - A review fails if:**

- ‚ùå Approved spec without error handling strategy
- ‚ùå Didn't verify observability plan (logging, PostHog events)
- ‚ùå Missed hardcoded dependencies (should use Constructor DI)
- ‚ùå Approved public interface without JSDoc documentation
- ‚ùå Missed `any` types or lack of Zod schemas for external data
- ‚ùå Didn't flag overly complex or prematurely abstracted design
